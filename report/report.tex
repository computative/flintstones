\documentclass[11pt,english,a4paper]{article}
\usepackage{babel}
\input{/home/marius/Dokumenter/preamples/phys_en.pre}
\author{\normalsize Marius Jonsson (Institutt for Vanskelig Fysikk, Oscars gate 19, 0352 OSLO, Norway) \\\\
\vspace{5px}
\normalsize \texttt{http://github.com/kingoslo/flintstones}}
\title{\bf \uppercase{Introduction to the Hartree-Fock method for quantum dots and preparations for variational monte-carlo.}}
\date{\normalsize \today}
\addbibresource{/home/marius/Dokumenter/MyLibrary.bib}
\DeclareUnicodeCharacter{2212}{$-$}
\begin{document}
\maketitle
\begin{abstract} \normalsize This is a report submission for the first project of «Computational physics 2» at the Institute of Physics, University of Oslo, autumn 2016.
\end{abstract}
\lstset{
  xleftmargin=.2\textwidth, xrightmargin=.2\textwidth
}

In this project we completed a variety of tasks to prepare ourselves for the final term project. We set up basis for compact subsets of $\mathbb{R}^2$ using quantum harmonic oscillator state functions. We did this by making a $\alpha \mapsto (n_x,n_y)$ bijection from the set of natural numbers to the two-tuples of harmonic oscillator principal quantum numbers on $\mathbb{R}^2$. In the class we included method to evaluate any given state basis function at the point $x \in \mathbb{R}^2$ and a method to return single Hermite polynomials. The latter was useful when we went ahead to build a Gaussian-quadrature Hermite integrator. The intent was that we would use this integrator to compute Coulomb interaction matrix elements,  $\langle \alpha \beta | V | \gamma\delta \rangle$, for the quantum dot variety of the Hartree-Fock method. However, halfway through the project period, the course administration decided to supply a function which calculated the matrix elements in polar coordinates. The Hermite integrator was therefore never implemented further than for the integration of polynomials. Finally we wrote a program to solve the induced Hartree-Fock equations. Suppose $\mu$ is the Fermi level of some quantum dot. We wanted to find a $N \times N$-matrix $C$ and an $N$-tuple $(\varepsilon_1, \cdots, \varepsilon_N)$ such that
\begin{equation}
\sum_{  \beta = 1}^N F_{  \alpha \beta} C_{  \beta \gamma} = e_\gamma C_{  \gamma \alpha} \qquad \text{for} \qquad F_{  \alpha \beta} = \varepsilon_{  \alpha} \delta_{  \alpha \beta} + \sum_{  j=1}^\mu \sum_{  \gamma = 1}^N \sum_{  \delta = 1}^N C_{  j \gamma}^* C_{  j \delta} \Big( \langle \alpha \gamma | V | \beta \delta \rangle - \langle \alpha \gamma | V | \delta \beta \rangle \Big).
\label{eq:HF}
\end{equation}
We will call these equations the Hartree-Fock equations.
\\
\\
The report is structured by «introduction»-, «methods»-, «results and discussion»- and finally a «conclusion and perspectives»-sections.
\section*{\uppercase{Methods}}
As we noted, the project required the completion of a variety of tasks. At the time of writing, it was unclear that there was any overarching methods. We will therefore use this section to present how the tasks were completed.\\
\\
We set up the harmonic oscillator is several steps. First we used the energy function 
\[
E(n_x,n_y) = \hbar \omega \left( n_x + n_y + 1 \right) \qquad \text{\parencite[92]{sakurai_modern_2011}\parencite[190]{griffiths_introduction_2005} }
\]
Clearly, this uniquely determines a hierarchy of groups of states ordered by energy. Assuming the particles were fermions we allowed exactly two spin projections for all combinations of $n_x$ and $n_y$. As you may recall, we were not satisfied by a group hierarchy: From this we constructed a bijection $\alpha \mapsto (n_x,n_y)$ from the set of natural numbers using a dictionary type ordering \parencite[26]{munkres_topology_2000} on $\mathbb{N} \times \mathbb{N}$. We present a table in the results. We will also present the assignment rule for the bijection in the case of polar coordinates.\\
\\
As explained, it was natural to implement the Hermite polynomials into the class. We know that the Hermite polynomials are of the form $ H_n(x) = \sum_{  m=0}^n a_m (2x)^m $. Under this hypothesis, insertion in the recurrence relation $H_{  n+1}(x) = 2x H_n(x) - 2n H_n(x)$ straightforwardly reduces to
\[
H_n(x) = n! \sum_{m=0}^{\floor{n/2}} \frac{(-1)^m}{m!(n-2m)!}(2x)^{n-2m},
\]
in the case we treat $n$ as being even and odd separately. Here we mean that $\floor{\cdot}$ denotes the floor function on $\mathbb{R}$. Due to separation of variables, and according to \cite[93]{sakurai_modern_2011}, the harmonic oscillator state functions are
\[
\psi_{  n_x,n_y}(x,y) = \psi_{  n_x}(x)\psi_{  n_y}(y), \qquad \psi_{n}(x) = \left( \frac{m \omega}{\pi \hbar} \right)^{1/4} \frac{H_n( (m \omega/\hbar)^{1/2} x )}{(2^n n!)^{1/2}}\exp\left( - m \omega/(2\hbar) x^2 \right).
\]
Having deduced an expression for $H_n$, implementing this into any harmonic basis for quantum dots is straightforward.\\
\\
The next task which was initiated was to implement a Gaussian quadrature algorithm. \begin{theorem}[Gaussian quadrature]
Suppose $A \subseteq \mathbb{R}$ and there exist an orthogonal basis $\{H_n\}_{n=0}^\infty$ of polynomials for the set of square integrable functions on $A$ with respect to the inner product
\[
\langle f,g\rangle = \int_A (Wfg)(x)\, \diff x.
\]
Suppose also that $H_n$ is a degree $n$-polynomial and $|\langle H_n,H_n \rangle| = c_n$. If $f: \mathbb{R} \to \mathbb{R}$ is integrable on $A$ and there exist and $N \in \mathbb{N}$ such that $f(x) = (WP_{2N-1})(x)$, then 
\[
\int_A f(x)\, \diff x = c_0 \sum_{i=1}^N (H^{-1})_{0n} P_{2N-1}(x_n),
\]
where $\{x_n\}_{n=1}^N$ are the zeros of $H_N$ and $(H^{-1})_{0n}$ is the inverse of the matrix with elements $H_{nk} = H_k(x_n)$.
\end{theorem} Proof of the theorem is contained in the appendix. This method was implemented according to the theorem. In particular, as the course administration decided to switch to polar coordinates, an analytical expression was supplied, and the integrator was therefore only used to integrate holomorphic functions on $\mathbb{R}$, where according to \cite[9,18]{stein_complex_2003} and \ref{thm:quad}, the integrator is stable. However, since the integrand of $\langle \alpha \beta | V | \gamma\delta \rangle$ is not holomorphic, I am not able to argue that the integrator is suitable to compute $\langle \alpha \beta | V | \gamma\delta \rangle$. Lastly, we were encouraged to work out the expression for $\langle \alpha \beta | V | \gamma\delta \rangle$. Suppose the principal quantum numbers of two-electron state indexed by $t$ are $n_t,m_t$, $V(r) = -e^2/(4\pi^2\varepsilon_0 r)$ denote the Coloumb potential, $\alpha,\beta,\gamma,\delta \in \{1,2,\cdots\}$ and let $A=\{ \alpha,\beta,\gamma,\delta  \}$, $B = \{(1,\alpha),(2,\beta),(1,\delta),(2,\gamma)\}$ then a parameterisation of the matrix element $\langle \alpha \beta | V  |  \gamma \delta \rangle$ is
\begin{align*}
\langle \alpha \beta | V  |  \gamma \delta \rangle &= \iiiint_{\mathbb{R}^4} \psi^*_\alpha(\vec{r}_1)\psi^*_\beta(\vec{r}_2)V(\| \vec{r}_1-\vec{r}_2 \|)\psi^*_\delta(\vec{r}_1)\psi^*_\gamma(\vec{r}_2)\,\diff \vec{r}_1 \, \diff \vec{r}_2\\
&= - C \int_{-\infty}^\infty\int_{-\infty}^\infty\int_{-\infty}^\infty\int_{-\infty}^\infty \prod_{(k,t)\in B} H_{n_t}(u_k) H_{m_t}(v_k)\frac{\exp\Big(\sum_{i=1}^2 u_i^2 + v_i^2 \Big)\diff u_1 \diff v_1 \diff u_2 \diff v_2}{([u_1 - u_2]^2 + [v_1 - v_2]^2)^{1/2}}, \\
&\text{for normalization factor} \quad C = \frac{\omega^{1/2} e^2}{4 \pi^3 \varepsilon_0}\Big( 2^{\sum_{i \in A} n_i + m_i}\prod_{j \in A}n_j!m_j! \Big)^{  -1/2}.
\end{align*}
\[
n(\alpha) = \frac{1}{2} \left[ \left(\frac{E}{\hbar \omega} \right)(\alpha) - |m(\alpha)| - 1 \right]\]
\[ \quad m(\alpha) = - \left| \left(\frac{E}{\hbar \omega} \right)(\alpha) - 1 \right| + 2 \floor{ \frac{1}{2}\left[\alpha - 1 - \left( \left(\frac{E}{\hbar \omega} \right)(\alpha) - 1 \right)\left(\frac{E}{\hbar \omega} \right)(\alpha) \right] }
\]
\[
\left(\frac{E}{\hbar \omega} \right)(\alpha) = \ceil{\frac{1}{2}\left( 1 + 4\alpha \right)^{1/2} - \frac{1}{2} }
\]
\[
A(m,l) = \ceil{ \left( \frac{m+l}{2} \right)\left( \frac{m+l}{2} -1 \right) } + (m+l) - \max(m,l)
\]
Lastly, we solved the Hartree-Fock equations \ref{eq:HF} by iteration. All $\langle \alpha \beta | V  |  \gamma \delta \rangle$ were pre calculated. It is easy to see that if one let 
\[
V_{  \alpha \beta \gamma \delta} = \langle \alpha \beta | V  |  \gamma \delta \rangle - \langle \alpha \beta | V  |  \delta \gamma \rangle,
\]
Then $V_{  \alpha \beta \gamma \delta}$ is anti-symmetric in its two first and last indecies:
\[
V_{  \alpha \beta \gamma \delta} = -V_{  \beta\alpha  \gamma \delta} = -V_{  \alpha \beta \delta \gamma } = V_{  \beta \alpha \delta\gamma } = V_{  \beta \alpha \delta\gamma },
\]
Moreover since $\psi_n$ can be made real
\[
V_{  \alpha \beta \gamma \delta} = V_{  \gamma \delta\alpha \beta}.
\]
In addition, we can show that $V_{  \alpha \beta \gamma \delta} = 0$ whenever the total spin of $ |  \alpha \beta \rangle $ is different from the total spin of $ |  \gamma  \delta \rangle $. Similarly $V_{  \alpha \beta \gamma \delta} = 0$ whenever the total angular momentum of $ |  \alpha \beta \rangle $ is different from the total spin of $ |  \gamma  \delta \rangle$. For some choices of $\mu$ and $N$, this reduced the number of stored matrix by a few orders of magnitudes, since we could compute only a few elements, and let the algorithm use the symmetries still determine the correct matrix elements. Define for brevity $\rho_{ \gamma \delta} = \sum_{j=1}^\mu C_{  j \gamma}^* C_{  j \delta}$In this notation, the Hartree-Fock equations become:
\[
\sum_{  \beta = 1}^N F_{  \alpha \beta} C_{  \beta \gamma} = e_\gamma C_{  \gamma \alpha} \qquad \text{for} \qquad F_{  \alpha \beta} = \varepsilon_{  \alpha} \delta_{  \alpha \beta} + \sum_{  \gamma = 1}^N \sum_{  \delta = 1}^N  \rho_{ \gamma \delta}V_{  \alpha \gamma \beta \delta}
\]
As we explained, the equations were solved iteritatively. We let the value of $C_{  \alpha \beta} = \text{diag}(1,1,\cdots 1,0,0,\cdots,0)$, where the number of one-elements were exactly $\mu$. Using this candidate for $C_{  \alpha \beta}$, $F_{  \alpha \beta}$ was determined for each iteration. Thereafter, the matrix $F$ was diagonalized $F = CDC^{  -1}$, where $D = \text{diag} ( \varepsilon_1, \cdots, \varepsilon_N)$. Such a decomposition exists since $F$ has exactly $N$ eigenvalues and $\varepsilon_j$ are the single state energies since the columns of $C$ are eigenvectors of $F$ by The Diagonalization theorem of linear algebra \parencite[282]{lay_linear_2012}. Clearly then, the algorithm is implemented by a 5-dimensional loop. One for each index $\alpha, \beta, \gamma, \delta$, and one loop until the desired precision is obtained. For brevity, the please see the github-page for the implementation: \texttt{http://github.com/kingoslo/flintstones}.\\
\\
Much effort was made to ensure that the code ran correctly. As we will see in the results, the code produced the correct energies for a number of values of $\mu$ an $N$.

\section*{\uppercase{Results and discussion}}
We introduce the results in the order they appeared in the project description.\\
\\
\texttt{(nx,ny,spin,energy)}:
\lstinputlisting{../resources/output.txt}
Find the energy for 6,12,20,30 elektroner
\texttt{(nx,ny,spin,energy)}:
\lstinputlisting{../resources/output.txt}

Integrator. Which quantum numbers are conserved? what are the possible total spin values?

\begin{prop}
The Hartree-Fock basis is orthonormal. \label{thm:hforth}
\end{prop}
\begin{proof}
It is easy to see that the two dimensional Harmonic oscillator functions are orthonormal. To see this, just write up $\langle \psi_{  \alpha} | \psi_{  \beta} \rangle$ and factor each state function into a product of 1 dimensional state functions. Therefore
\begin{equation}
\int_{- \infty}^\infty\int_{- \infty}^\infty \psi_\alpha^*(x,y)  \psi_\beta(x,y) \, \diff x \, \diff y = \delta_{  \alpha\beta} = \iint_{  \mathbb{R}^2} \psi_{  \alpha}^* \psi_{  \beta} \, \diff \sigma = \langle \alpha|\beta \rangle =\delta_{  \alpha \beta} \label{eq:orthnm}
\end{equation}
Moreover, since the matrix elements $C$ are unitary,
\begin{equation}
\sum_{\alpha=1}^\infty (C^\dagger)_{  \alpha p }C_{q\alpha} = \delta_{  pq} \label{eq:unitary}
\end{equation}
\begin{align*}
\langle \psi_{  p} | \psi_{  q} \rangle &= \iint_{  \mathbb{R}^2} \psi_{  p}^* \psi_{  q} \, \diff \sigma = \int_{- \infty}^\infty\int_{- \infty}^\infty \psi_{  p}^*(x,y) \psi_{  q}(x,y) \, \diff x \, \diff y \\
&= \int_{- \infty}^\infty\int_{- \infty}^\infty \Big[ \sum_{\alpha=1}^\infty C_{p\alpha} \psi_\alpha(x,y) \Big]^* \Big[ \sum_{\beta=1}^\infty C_{q\beta} \psi_\beta(x,y) \Big] \, \diff x \, \diff y \\
&= \sum_{\alpha=1}^\infty\sum_{\beta=1}^\infty C_{p\alpha}^*C_{q\beta}\undercbrace{\int_{- \infty}^\infty\int_{- \infty}^\infty \psi_\alpha^*(x,y)  \psi_\beta(x,y) \, \diff x \, \diff y}_{= \delta_{  \alpha\beta} \quad \eqref{eq:orthnm}}  \stackrel{\eqref{eq:orthnm}}{=} \sum_{\alpha=1}^\infty\sum_{\beta=1}^\infty C_{p\alpha}^*C_{q\beta}\delta_{  \alpha\beta} \\
&= \sum_{\alpha=1}^\infty C_{p\alpha}^*C_{q\alpha} 
= \sum_{\alpha=1}^\infty (\undercbrace{C_{p\alpha}}_{={C\T}_{\alpha p }})^*C_{q\alpha} = \sum_{\alpha=1}^\infty ({C\T}_{\alpha p })^*C_{q\alpha} = \sum_{\alpha=1}^\infty (C^\dagger)_{  \alpha p }C_{q\alpha} \stackrel{\eqref{eq:unitary}}{=} \delta_{  pq},
\end{align*}
which proves that the Hartree-Fock basis is orthonormal
\end{proof}

\begin{prop}
Let $\psi_{  p_1\cdots p_n}(\vec{r}_1,\cdots, \vec{r}_n)$ and $\psi_{  \beta_1\cdots \beta_n}(\vec{r}_1,\cdots, \vec{r}_n)$ denote the Hartree-Fock and harmonic oscillator slater determintants respectively, then:
\[
\psi_{  p_1\cdots p_n}(\vec{r}_1,\cdots, \vec{r}_n) = \det (C) \psi_{  \beta_1\cdots \beta_n}(\vec{r}_1,\cdots, \vec{r}_n) 
\] \label{prop:slater}
\end{prop}
\begin{proof}
Let's agree to write $\psi_{p_i}(x_j,y_j) = \psi_{p_i}(\vec{r}_j) = \psi_{  p_ij}$. Now suppose $A$ and $B$ are matrices. Then we know that $\det(AB) = \det(A)\det(B)$ $(\dagger)$ by the multiplicative determinant theorem \parencite[173]{lay_linear_2012}. Therefore it clearly follows that $\psi_{  p_1\cdots p_n}(\vec{r}_1,\cdots, \vec{r}_n) = \det(C)\psi_{  \alpha_1\cdots \alpha_n}(\vec{r}_1,\cdots, \vec{r}_n)$. To see this write
\begin{align*}
\psi_{  p_1\cdots p_n}(\vec{r}_1,\cdots, \vec{r}_n) &=  \frac{1}{(n!)^{1/2}}
\begin{vmatrix}
\psi_{  p_1 1} & \psi_{  p_1 2} & \cdots & \psi_{  p_1 n}\\
\psi_{  p_2 1} & \psi_{  p_2 2} & \cdots & \psi_{  p_2 n}\\
\vdots & \ddots && \vdots\\
\psi_{  p_n 1} & \psi_{  p_n 2} & \cdots & \psi_{  p_n n}
\end{vmatrix}\\
&=
\frac{1}{(n!)^{1/2}}\begin{vmatrix}
\sum_{  \beta_1=1}^\infty C_{  p_1\beta_1}\psi_{  \beta_1 1} & \sum_{  \beta_1=1}^\infty C_{  p_1\beta_1}\psi_{  \beta_1 2} & \cdots & \sum_{  \beta_1=1}^\infty C_{  p_1\beta_1}\psi_{  \beta_1 n}\\
\sum_{  \beta_2=1}^\infty C_{  p_2\beta_2}\psi_{  \beta_2 1} & \sum_{  \beta_2=1}^\infty C_{  p_2\beta_2}\psi_{  \beta_2 2} & \cdots & \sum_{  \beta_2=1}^\infty C_{  p_2\beta_2}\psi_{  \beta_2 n}\\
\vdots & \ddots && \vdots\\
\sum_{  \beta_n=1}^\infty C_{  p_n\beta_n}\psi_{  \beta_n 1} & \sum_{  \beta_1=1}^\infty C_{  p_n\beta_n}\psi_{  \beta_n 2} & \cdots & \sum_{  \beta_n=1}^\infty C_{  p_n\beta_n}\psi_{  \beta_n n}\\
\end{vmatrix}\\
&=
\frac{1}{(n!)^{1/2}}\begin{vmatrix}
(C \psi)_{  \beta_1 1} & (C \psi)_{  \beta_1 2} & \cdots & (C \psi)_{  \beta_1 n}\\
(C \psi)_{  \beta_2 1} & (C \psi)_{  \beta_2 2} & \cdots & (C \psi)_{  \beta_2 n}\\
\vdots & \ddots && \vdots\\
(C \psi)_{  \beta_n 1} & (C \psi)_{  \beta_n 2} & \cdots & (C \psi)_{  \beta_n n}
\end{vmatrix} \stackrel{(\dagger)}{=} \det(C)\psi_{  \beta_1\cdots \beta_n}(\vec{r}_1,\cdots, \vec{r}_n),
\end{align*}
which proves the proposition. 
\end{proof}
Lets show two corollaries of this proposition. We first show a little lemma
\begin{lemma}
Suppose $C$ is unitary over $\mathbb{C}$. Then there exist a $\theta \in (-\pi,\pi]$ such that $\det(C) = e^{ i \theta}$. \label{thm:unitary}
\end{lemma}
\begin{proof}
Since by the Leibniz formula, $\det (C) \in \mathbb{C}$ and hence there exist $\theta \in (-\pi,\pi]$ and $r>0$ such that $\det C = r e^{ i \theta}$. Therefore $r = 1$ if and only if $|\det(C)| = 1$. Since we know that for any matrix $ \det A = \det A\T $ $(*)$ \parencite[172]{lay_linear_2012}. Moreover as a corollary of the Leibniz formula, $\det (A^*) = (\det A)^*$ $(**)$. We know that the determinant of the identity is 1, so we can write
\begin{align}
1 &= \det(I) = \det(C^\dagger C) \stackrel{(\dagger)}{=} \det(C^\dagger)\det( C) = \det\left( (C^*)\T \right) \det (C) \nonumber \\
&\stackrel{(*)}{=} \det\left( C^* \right) \det (C) \stackrel{(**)}{=}\det\left( C \right)^* \det (C) = |\det (C)|^2 \qquad \text{only if} \qquad  |\det(C)| = \pm 1, \label{eq:unitarynorm}
\end{align}
But since we saw that $\det C = r e^{ i \theta}$, we get
\[
0<r = |r| = |r| |e^{ i \theta}| = |r e^{ i \theta}| = |\det (C)| \stackrel{\eqref{eq:unitarynorm} }{=} \pm 1,
\]
and the lemma follows.
\end{proof}
In particular, the norm of the determinant is 1. To obtain the first corollary is easy now since it is
\begin{corollary}
Assume $\psi_{  p_1\cdots p_n}(\vec{r}_1,\cdots, \vec{r}_n)$ and $\psi_{  \beta_1\cdots \beta_n}(\vec{r}_1,\cdots, \vec{r}_n)$ denote the Hartree-Fock and harmonic oscillator slater determintants respectively, then there exist an $\alpha \in \mathbb{R}$ such that.
\[
\psi_{  p_1\cdots p_n}(\vec{r}_1,\cdots, \vec{r}_n) = e^{ i \theta}\psi_{  \beta_1\cdots \beta_n}(\vec{r}_1,\cdots, \vec{r}_n) 
\]
\end{corollary}
\begin{proof}
By proposition \ref{prop:slater} we have that
\[
\psi_{  p_1\cdots p_n}(\vec{r}_1,\cdots, \vec{r}_n) = \det (C) \psi_{  \beta_1\cdots \beta_n}(\vec{r}_1,\cdots, \vec{r}_n) 
\]
Now since $C$ is unitary we know by lemma \ref{thm:unitary} that there exist an $\alpha \in \mathbb{R}$ such that $\det C = e^{ i \alpha}$.
\end{proof}
The second corollary is a also very easy,
\begin{corollary}
Assume $\psi_{  p_1\cdots p_n}(\vec{r}_1,\cdots, \vec{r}_n)$ and $\psi_{  \beta_1\cdots \beta_n}(\vec{r}_1,\cdots, \vec{r}_n)$ denote the Hartree-Fock and harmonic oscillator slater determintants respectively. Then $\psi_{  p_1\cdots p_n}(\vec{r}_1,\cdots, \vec{r}_n)$ is normalized if and only if $\psi_{  \beta_1\cdots \beta_n}(\vec{r}_1,\cdots, \vec{r}_n)$ is normalized regardless of the choice of norm.
\end{corollary}
\begin{proof}
By the corollary we know that
\[
\psi_{  p_1\cdots p_n}(\vec{r}_1,\cdots, \vec{r}_n) = e^{ i \theta}\psi_{  \beta_1\cdots \beta_n}(\vec{r}_1,\cdots, \vec{r}_n) 
\]
Now suppose that $\|\cdot \|$ is any norm, then $\|a \psi \| = |a| \| \psi \|$ $(/)$ \parencite[124]{lindstrom_mathematical_2016}. Then
\begin{align*}
\|\psi_{  p_1\cdots p_n}(\vec{r}_1,\cdots, \vec{r}_n) \| = \|e^{ i \theta}\psi_{  \beta_1\cdots \beta_n}(\vec{r}_1,\cdots, \vec{r}_n)\| \stackrel{(/)}{=} |e^{ i \theta}|\|\psi_{  \beta_1\cdots \beta_n}(\vec{r}_1,\cdots, \vec{r}_n)\| = \|\psi_{  \beta_1\cdots \beta_n}(\vec{r}_1,\cdots, \vec{r}_n)\|
\end{align*}
And hence $\psi_{  \beta_1\cdots \beta_n}(\vec{r}_1,\cdots, \vec{r}_n)$ is normalized if and only if $\psi_{  p_1\cdots p_n}(\vec{r}_1,\cdots, \vec{r}_n)$ is normalized.
\end{proof}

Vis siste avsnitt 1d Se slide side 19. Skriv opp basisskiftet funksjonal, deriver. Ferdig.
\begin{theorem}[Hartree-Fock equations]
Suppose $E = E(C_{  \alpha \beta})$ is the energy of $\psi_{  p_1\cdots p_n}(\vec{r}_1,\cdots, \vec{r}_n)$. Then $\psi_{  p_1\cdots p_n}(\vec{r}_1,\cdots, \vec{r}_n) = \det (C) \psi_{  \beta_1\cdots \beta_n}(\vec{r}_1,\cdots, \vec{r}_n)$ and $E$ is minimized if $C_{  \alpha \beta}$ satisfy
\begin{equation}
\sum_{  \beta = 1}^N F_{  \alpha \beta} C_{  \beta \gamma} = e_\gamma C_{  \gamma \alpha} \qquad \text{for} \qquad F_{  \alpha \beta} = \varepsilon_{  \alpha} \delta_{  \alpha \beta} + \sum_{  \gamma = 1}^N \sum_{  \delta = 1}^N  \rho_{ \gamma \delta}V_{  \alpha \gamma \beta \delta}. \label{eq:hfeq}
\end{equation}
\end{theorem}
\begin{proof}
Note that we already saw that by proposition \ref{prop:slater}, $\psi_{  p_1\cdots p_n}(\vec{r}_1,\cdots, \vec{r}_n) = \det (C) \psi_{  \beta_1\cdots \beta_n}(\vec{r}_1,\cdots, \vec{r}_n)$ is automatically satisfied. Therefore it suffices to show that $E$ is minimized if $C_{  \alpha \beta}$ satisfy \eqref{eq:hfeq}. This is exactly the conclusion of the Lagrange multiplier theorem for a finite set of constrains. By proposition \ref{thm:hforth}, the basis functions $ |  i \rangle$ will automatically be orthogonal. Therefore it suffices to minimize the energy with respect to the normalization constraint $\langle i|i \rangle = 1$. We call the Lagrange multipliers $e_i$ for $1 \leq i \leq \mu$ and write the minimizing condition according to Lagrange multiplier theorem:
\begin{align*}
0 &= \pp ;C_{  i \alpha}; \left[ E(C_{  i \alpha}) - \sum_{i=1}^\mu e_i(\langle i|i \rangle(C_{  i \alpha}) - 1) \right] \\
&= \pp ;C_{  i \alpha}; \left[ 
\sum_{i=1}^\mu \langle i | h_0 | i \rangle + \frac{1}{2} \sum_{i=1}^\mu \sum_{j=1}^\mu \left[ \langle ij|v|ij \rangle - \langle ij|v|ji \rangle \right]
- \sum_{i=1}^\mu e_i\langle i|i \rangle(C_{  i \alpha} )
\right] + \undercbrace{\pp ;C_{  i \alpha}; \sum_{i=1}^\mu e_i}_{  =0}\\
&=  
\sum_{i=1}^\mu \sum_{\alpha=1}^N \sum_{\beta=1}^N\pp ;C_{  i \alpha}; C^*_{  i\alpha} C_{  i \beta} \langle \alpha | h_0 | \beta \rangle - \sum_{i=1}^\mu\sum_{\alpha=1}^N \pp ;C_{  i \alpha};e_iC^*_{  i\alpha} C_{  i \alpha}\\
&+ \frac{1}{2} \sum_{i=1}^\mu \sum_{j=1}^\mu \sum_{\alpha=1}^N\sum_{\beta=1}^N\sum_{\gamma=1}^N\sum_{\delta=1}^N\pp ;C_{  i \alpha}; C^*_{  i\alpha} C_{  j \beta}^*C_{  i\gamma} C_{  j \delta} \undercbrace{\left[ \langle \alpha \beta|v|\gamma \delta \rangle - \langle \alpha \beta|v|\delta \gamma \rangle \right]}_{=V_{  \alpha \beta \gamma \delta}} \\
&= \sum_{\beta=1}^N \left[ \varepsilon \delta_{  \alpha \beta} + \sum_{\gamma = 1}^N \sum_{\delta=1}^N \rho_{  \gamma \delta} V_{  \alpha \gamma \beta \delta} \right] C_{  i \beta} - e_i C_{  i \alpha}
\end{align*}
Now add $e_i C_{  i \alpha}$ to each side of the equation, and the theorem follows.
\end{proof}

\begin{tabular}{c | c c c c c c c c}
$R$ & \multicolumn{8}{c}{$N$}\\
  & 2         & 4         & 6         & 8         &12         &14        & 18       & 20\\
  \hline
4 & 3.1626916 & 10.747470 & 20.766927 & 34.829996 & 70.673854 & 92.85253 & 145.9196 & 177.9632\\
5 & 3.1619219 & 10.745515 & 20.748418 & 34.266168 & 67.569938 & 89.21037 & 139.7442 & 168.5296\\
6 & 3.1619219 & 10.744865 & 20.720248 & 34.184146 & 67.296902 & 87.85635 & 134.8867 & 161.3397\\
7 & 3.1619096 & 10.744678 & 20.720123 & 34.152530 & 66.934710 & 87.33664 & 133.9035 & 159.9586\\
9 & 3.1619143 & 10.744446 & 20.719239 & 34.152248 & 66.912283 & 87.13293 & 132.8712 & 158.2259\\
13& -         & -         & -         & -         & -         & -        & -        & -       \\
\end{tabular}

Hvor mange singlepartikkel tilstander trenger vi før hartree fock energien stabiliseres? Kalles Hartree fock limit

Sammenlign hartree fock energi og uforstyrret energi

How much do the single-particle energies change compared to the harmonic
oscillator energies? 

Are the degeneracies seen in the harmonic oscillator calculations preserved?

\section*{\uppercase{Conclusion and perspectives}}

\section*{\uppercase{Appendix}}
\begin{theorem}[Gaussian quadrature]
Suppose $A \subseteq \mathbb{R}$ and there exist an orthogonal basis $\{H_n\}_{n=0}^\infty$ of polynomials for the set of square integrable functions on $A$ with respect to the inner product
\[
\langle f,g\rangle = \int_A (Wfg)(x)\, \diff x.
\]
Suppose also that $H_n$ is a degree $n$-polynomial and $|\langle H_n,H_n \rangle| = c_n$. If $f: \mathbb{R} \to \mathbb{R}$ is integrable on $A$ and there exist and $N \in \mathbb{N}$ such that $f(x) = (WP_{2N-1})(x)$, then 
\[
\int_A f(x)\, \diff x = c_0 \sum_{i=1}^N (H^{-1})_{0n} P_{2N-1}(x_n),
\]
where $\{x_n\}_{n=1}^N$ are the zeros of $H_N$ and $(H^{-1})_{0n}$ is the inverse of the matrix with elements $H_{nk} = H_k(x_n)$. \label{thm:quad}
\end{theorem}
\begin{proof}
Assume that the hypothesis is true, then in particular
\begin{equation}
f(x) = (WP_{2N-1})(x) \label{eq:product}
\end{equation}
Since $\{H_n\}_{n=1}^\infty$ is a polynomial basis for the space of square integrable $\mathbb{R} \to \mathbb{R}$-functions, there exist polynomials $Q_{N-1}$ and $R_{N-1}$, such that
\begin{equation}
P_{2N-1}(x) = H_N(x) R_{N-1} (x) + Q_{N-1}(x) = H_N(x) \sum_{k=0}^{N-1}r_nH_k(x) + \sum_{k=0}^{N - 1}q_k H_{k}(x) \label{eq:polynomial}
\end{equation}
Moreover, since the basis is orthogornal with respect to the given inner product, there exist normalization $c_{m}$ such that
\begin{equation}
\langle H_n,H_m\rangle = \int_A W(x) H_n(x) H_m(x)\, \diff x = c_{m}\delta_{mn}. \label{eq:orthonormality}
\end{equation}
Therefore the integral of interest is
\begin{align}
\int_A f(x) \,\diff x &\stackrel{\ref{eq:product}}{=} \int_A W(x)P_{2N-1}(x) \,\diff x \stackrel{\eqref{eq:polynomial}}{=} \int_A W(x)\left[ H_N(x) \sum_{k=0}^{N-1}r_nH_k(x) + \sum_{k=0}^{N - 1}q_k H_{k}(x) \right] \,\diff x \nonumber \\
&\stackrel{\eqref{eq:orthonormality}}{=} 0 + \sum_{k=0}^{N - 1}\int_A W(x) q_k H_{k}(x) \,\diff x = \sum_{k=0}^{N - 1} q_k \int_A W(x) H_{k}(x)\cdot\undercbrace{1}_{=H_0} \,\diff x \stackrel{\eqref{eq:orthonormality}}{=} \sum_{k=0}^{N - 1} q_kc_{k} \delta_{k0} \nonumber \\
&=q_0c_{0} \label{eq:coeff}
\end{align}
Since $H_N(x)$ is a degree $N$ polynomial by assumption, $H_N$ has exactly $N$ zeros by the fundamental theorem of algebra \parencite[12]{forster_lectures_1991}. Therefore there exist a set $\{x_k\}_{k=1}^N$ such that $H_N(x_k)=0$ for all $1 \leq k \leq N$. Define now $c_n = Q_{N-1}(x_n)$ and observe that
\begin{equation}
c_n = Q_{N-1}(x_n) \stackrel{\eqref{eq:polynomial}}{=} \sum_{k=0}^{N-1}q_k H_k(x_n) \equiv \sum_{k=0}^{N-1}q_k H_{nk} \label{eq:cn}
\end{equation}
Since $\{H_n\}$ is a basis, each element is linearly independent, and therefore the matrix consisting of elements $H_{nk}$ is invertible with inverse $(H^{-1})_{mn}$. By solving for $b_k$ we obtain
\begin{equation}
\sum_{n=0}^{N-1}(H^{-1})_{mn}Q_{N-1}(x_n) = \sum_{n=0}^{N-1}(H^{-1})_{mn} c_n \stackrel{\eqref{eq:cn}}{=} \sum_{k=0}^{N-1}\sum_{n=0}^{N-1}q_k(H^{-1})_{mn} H_{nk} = \sum_{k=0}^{N-1}q_k \delta_{mk} = q_{m} \label{eq:qm}
\end{equation}
But since $\{x_k\}$ are the zeros ($\dagger$) of $H_N$, we see that
\begin{align}
q_m &\stackrel{\eqref{eq:qm}}{=} \sum_{n=0}^{N-1}(H^{-1})_{mn}Q_{N-1}(x_n) \stackrel{\eqref{eq:polynomial}}{=} \sum_{n=0}^{N-1}(H^{-1})_{mn}\Big[ P_{2N-1}(x_n) - \undercbrace{H_N(x_n)}_{=0 \quad (\dagger) } R_{N-1} (x_n) \Big] \nonumber\\
& \stackrel{(\dagger)}{=} \sum_{n=0}^{N-1}(H^{-1})_{mn}P_{2N-1}(x_n) \qquad \text{only if}\qquad q_0 = \sum_{n=0}^{N-1}(H^{-1})_{0n}P_{2N-1}(x_n). \label{q0}
\end{align}
By setting \eqref{q0} equal to \eqref{eq:coeff}, the theorem follows.
\end{proof}

\printbibliography
\end{document}